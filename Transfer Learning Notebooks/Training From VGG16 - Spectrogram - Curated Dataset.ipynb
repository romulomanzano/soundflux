{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urbansounds dataset proved to be very noisy. A lot of the classes were either irrelevant to our work or too similar for spectrograms to be distinguishable.\n",
    "\n",
    "We decided to narrow the focus of our modelling efforts by selecting a few categories that could be relevant to the task at hand.\n",
    "\n",
    "As such we selected a few classes from the FSD project - https://zenodo.org/record/2552860#.XIG9LMtKg5l and of course or own SoundFlux dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of March 7th, the classes used are as followes:\n",
    "\n",
    "- Falling Dummy (simulated human falls from Rescue Randy) from SoundFlux\n",
    "- General noise from SoundFlux\n",
    "- Telephone - from FSD\n",
    "- Laughter - from FSD\n",
    "- Knock - from FSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from: https://s3.amazonaws.com/soundflux-urbansounds/curated_4_class_bundle.zip (should be named '5 class bundle' ... will rename at some point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=80\n",
    "img_width = 256\n",
    "approx_fold_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/home/nvidia/bundle/split/train/\"\n",
    "test_folder = \"/home/nvidia/bundle/split/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            #rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.3,\n",
    "                            #horizontal_flip=True,\n",
    "                            #vertical_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\"\"\"   #Adding custom Layers\n",
    "   x = model.output\n",
    "   x = Flatten()(x)\n",
    "   x = Dense(4096, activation=\"relu\")(x)\n",
    "   x = Dropout(0.5)(x)\n",
    "   x = Dense(4096, activation=\"relu\")(x)\n",
    "   x = Dropout(0.5)(x)\n",
    "\"\"\"\n",
    "model = keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dense(64,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nclass, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 15,012,421\n",
      "Trainable params: 7,377,157\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.RMSprop(lr=0.0001,decay=1e-3)\n",
    "#opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weigh\"\"\"ts!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [train_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Found 432 images belonging to 5 classes.\n",
      "31/31 [============================>.] - ETA: 0s - loss: 1.2070 - acc: 0.4859Found 144 images belonging to 5 classes.\n",
      "32/31 [==============================] - 74s 2s/step - loss: 1.2054 - acc: 0.4862 - val_loss: 0.8299 - val_acc: 0.7144\n",
      "Epoch 2/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.7815 - acc: 0.7002 - val_loss: 0.4865 - val_acc: 0.8454\n",
      "Epoch 3/20\n",
      "32/31 [==============================] - 53s 2s/step - loss: 0.5555 - acc: 0.8086 - val_loss: 0.4244 - val_acc: 0.8351\n",
      "Epoch 4/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.4786 - acc: 0.8477 - val_loss: 0.2797 - val_acc: 0.9200\n",
      "Epoch 5/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.3904 - acc: 0.8681 - val_loss: 0.5339 - val_acc: 0.8384\n",
      "Epoch 6/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.3374 - acc: 0.8954 - val_loss: 0.3015 - val_acc: 0.9041\n",
      "Epoch 7/20\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.2572 - acc: 0.9111 - val_loss: 0.4473 - val_acc: 0.8794\n",
      "Epoch 8/20\n",
      "32/31 [==============================] - 53s 2s/step - loss: 0.2663 - acc: 0.9150 - val_loss: 0.3202 - val_acc: 0.9095\n",
      "Epoch 9/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.2270 - acc: 0.9277 - val_loss: 0.2839 - val_acc: 0.9052\n",
      "Epoch 10/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.2014 - acc: 0.9395 - val_loss: 0.3318 - val_acc: 0.9090\n",
      "Epoch 11/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.1761 - acc: 0.9454 - val_loss: 0.3410 - val_acc: 0.9213\n",
      "Epoch 12/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.1586 - acc: 0.9434 - val_loss: 0.3124 - val_acc: 0.9138\n",
      "Epoch 13/20\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.1832 - acc: 0.9541 - val_loss: 0.4695 - val_acc: 0.8761\n",
      "Epoch 14/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.1122 - acc: 0.9560 - val_loss: 0.4821 - val_acc: 0.8804\n",
      "Epoch 15/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.1320 - acc: 0.9580 - val_loss: 0.4384 - val_acc: 0.8869\n",
      "Epoch 16/20\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.0989 - acc: 0.9668 - val_loss: 0.2908 - val_acc: 0.9265\n",
      "Epoch 17/20\n",
      "32/31 [==============================] - 52s 2s/step - loss: 0.1119 - acc: 0.9658 - val_loss: 0.3652 - val_acc: 0.9170\n",
      "Epoch 18/20\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.1170 - acc: 0.9668 - val_loss: 0.3624 - val_acc: 0.9300\n",
      "Epoch 19/20\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.0855 - acc: 0.9668 - val_loss: 0.3749 - val_acc: 0.9200\n",
      "Epoch 20/20\n",
      "31/31 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9708"
     ]
    }
   ],
   "source": [
    "#RESET WEIGHTS!!\n",
    "#model.load_weights('raw_model.h5')\n",
    "#\n",
    "history = model.fit_generator(train_generator,\n",
    "                          steps_per_epoch=approx_fold_size/batch_size,\n",
    "                          validation_data = test_generator,\n",
    "                          validation_steps = approx_fold_size/batch_size,\n",
    "                          epochs=20,\n",
    "                          shuffle=True, \n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best so far -> lr = 0.0001, decay=1e-3, 13 epochs. Filal val accuracy = 92, loss = 0.4309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "model.evaluate_generator(test_generator,\n",
    "                              steps=approx_fold_size/batch_size,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('curated_model_v1_unfrozen_layers.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights were saved on the very first run of this model and ara available here: \n",
    "https://s3.amazonaws.com/soundflux-urbansounds/curated_model_v1.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
