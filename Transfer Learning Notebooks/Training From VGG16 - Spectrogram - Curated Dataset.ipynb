{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urbansounds dataset proved to be very noisy. A lot of the classes were either irrelevant to our work or too similar for spectrograms to be distinguishable.\n",
    "\n",
    "We decided to narrow the focus of our modelling efforts by selecting a few categories that could be relevant to the task at hand.\n",
    "\n",
    "As such we selected a few classes from the FSD project - https://zenodo.org/record/2552860#.XIG9LMtKg5l and of course or own SoundFlux dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of March 7th, the classes used are as followes:\n",
    "\n",
    "- Falling Dummy (simulated human falls from Rescue Randy) from SoundFlux\n",
    "- General noise from SoundFlux\n",
    "- Telephone - from FSD\n",
    "- Laughter - from FSD\n",
    "- Knock - from FSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from: https://s3.amazonaws.com/soundflux-urbansounds/curated_4_class_bundle.zip (should be named '5 class bundle' ... will rename at some point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=80\n",
    "img_width = 256\n",
    "approx_fold_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/home/nvidia/bundle/split/train/\"\n",
    "test_folder = \"/home/nvidia/bundle/split/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            #rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.3,\n",
    "                            #horizontal_flip=True,\n",
    "                            #vertical_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))\n",
    "base_model.trainable = False\n",
    "model = keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dense(64,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nclass, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 15,012,421\n",
      "Trainable params: 297,733\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.RMSprop(lr=0.001)\n",
    "#opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weigh\"\"\"ts!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [train_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Found 432 images belonging to 5 classes.\n",
      "31/31 [============================>.] - ETA: 0s - loss: 1.4815 - acc: 0.3085Found 144 images belonging to 5 classes.\n",
      "32/31 [==============================] - 68s 2s/step - loss: 1.4758 - acc: 0.3145 - val_loss: 1.2060 - val_acc: 0.5431\n",
      "Epoch 2/30\n",
      "32/31 [==============================] - 50s 2s/step - loss: 1.1306 - acc: 0.5283 - val_loss: 0.9084 - val_acc: 0.7050\n",
      "Epoch 3/30\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.9128 - acc: 0.6329 - val_loss: 0.7233 - val_acc: 0.7446\n",
      "Epoch 4/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.7878 - acc: 0.7207 - val_loss: 0.7354 - val_acc: 0.7226\n",
      "Epoch 5/30\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.7413 - acc: 0.7177 - val_loss: 0.6249 - val_acc: 0.7672\n",
      "Epoch 6/30\n",
      "32/31 [==============================] - 50s 2s/step - loss: 0.6603 - acc: 0.7461 - val_loss: 0.5091 - val_acc: 0.8297\n",
      "Epoch 7/30\n",
      "32/31 [==============================] - 50s 2s/step - loss: 0.6061 - acc: 0.7627 - val_loss: 0.6030 - val_acc: 0.7566\n",
      "Epoch 8/30\n",
      "32/31 [==============================] - 51s 2s/step - loss: 0.5879 - acc: 0.7939 - val_loss: 0.4603 - val_acc: 0.8265\n",
      "Epoch 9/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.5210 - acc: 0.8123 - val_loss: 0.4902 - val_acc: 0.8190\n",
      "Epoch 10/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.5112 - acc: 0.8212 - val_loss: 0.4887 - val_acc: 0.8235\n",
      "Epoch 11/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.4524 - acc: 0.8535 - val_loss: 0.4539 - val_acc: 0.8502\n",
      "Epoch 12/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.4155 - acc: 0.8632 - val_loss: 0.3898 - val_acc: 0.8534\n",
      "Epoch 13/30\n",
      "32/31 [==============================] - 50s 2s/step - loss: 0.4071 - acc: 0.8652 - val_loss: 0.4272 - val_acc: 0.8465\n",
      "Epoch 14/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.4001 - acc: 0.8740 - val_loss: 0.5732 - val_acc: 0.8050\n",
      "Epoch 15/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.3896 - acc: 0.8692 - val_loss: 0.4354 - val_acc: 0.8556\n",
      "Epoch 16/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.3475 - acc: 0.8847 - val_loss: 0.5352 - val_acc: 0.7971\n",
      "Epoch 17/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.3410 - acc: 0.8857 - val_loss: 0.4962 - val_acc: 0.8330\n",
      "Epoch 18/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.3509 - acc: 0.8760 - val_loss: 0.4168 - val_acc: 0.8750\n",
      "Epoch 19/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.2993 - acc: 0.8935 - val_loss: 0.5974 - val_acc: 0.8092\n",
      "Epoch 20/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.3325 - acc: 0.9023 - val_loss: 0.4065 - val_acc: 0.8599\n",
      "Epoch 21/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.2510 - acc: 0.9171 - val_loss: 0.6098 - val_acc: 0.8276\n",
      "Epoch 22/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.2687 - acc: 0.9141 - val_loss: 0.3732 - val_acc: 0.8772\n",
      "Epoch 23/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2686 - acc: 0.9121 - val_loss: 0.4342 - val_acc: 0.8448\n",
      "Epoch 24/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2577 - acc: 0.9150 - val_loss: 0.4812 - val_acc: 0.8438\n",
      "Epoch 25/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.2660 - acc: 0.9112 - val_loss: 0.5980 - val_acc: 0.8344\n",
      "Epoch 26/30\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2107 - acc: 0.9413 - val_loss: 0.4933 - val_acc: 0.8534\n",
      "Epoch 27/30\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.2667 - acc: 0.9141 - val_loss: 0.5490 - val_acc: 0.8103\n",
      "Epoch 28/30\n",
      "32/31 [==============================] - 47s 1s/step - loss: 0.2816 - acc: 0.9005 - val_loss: 0.4180 - val_acc: 0.8695\n",
      "Epoch 29/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.2069 - acc: 0.9444 - val_loss: 0.4912 - val_acc: 0.8599\n",
      "Epoch 30/30\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.1844 - acc: 0.9327 - val_loss: 0.5418 - val_acc: 0.8534\n"
     ]
    }
   ],
   "source": [
    "#RESET WEIGHTS!!\n",
    "#model.load_weights('raw_model.h5')\n",
    "#\n",
    "history = model.fit_generator(train_generator,\n",
    "                          steps_per_epoch=approx_fold_size/batch_size,\n",
    "                          validation_data = test_generator,\n",
    "                          validation_steps = approx_fold_size/batch_size,\n",
    "                          epochs=30,\n",
    "                          shuffle=True, \n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 images belonging to 5 classes.\n",
      "32/31 [==============================] - 26s 827ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6174462766482912, 0.84375]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "model.evaluate_generator(test_generator,\n",
    "                              steps=approx_fold_size/batch_size,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('curated_model_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights were saved on the very first run of this model and ara available here: \n",
    "https://s3.amazonaws.com/soundflux-urbansounds/curated_model_v1.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
