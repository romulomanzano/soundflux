{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning from InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this step is to leverage existing state of the art models such as Ince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed some details from this example [https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3](https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=128\n",
    "img_width = 150\n",
    "approx_fold_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_sounds_folder = \"/media/romulo/6237-3231/urban_sound_challenge/spectrograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_directories = []\n",
    "for i in range(1,11):\n",
    "    directory = urban_sounds_folder+\"/fold\"+str(i)\n",
    "    fold_directories.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold1',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold2',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold3',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold4',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold5',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold6',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold7',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold8',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold9',\n",
       " '/media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold10']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 2, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 21,823,274\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras import applications\n",
    "from keras import metrics\n",
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 10\n",
    "\n",
    "base_model = applications.InceptionV3(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(base_model)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "add_model.add(Dropout(0.5))\n",
    "add_model.add(Dense(nclass, \n",
    "                    activation='softmax'))\n",
    "\n",
    "model = add_model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.SGD(lr=1e-4, \n",
    "                                       momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weights!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1, holding data from /media/romulo/6237-3231/urban_sound_challenge/spectrograms/fold1 and training on the remaining 9\n",
      "Epoch 1/20\n",
      "Found 873 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Found 837 images belonging to 10 classes.\n",
      "Found 990 images belonging to 10 classes.\n",
      "Found 838 images belonging to 10 classes.\n",
      "Found 936 images belonging to 10 classes.\n",
      "Found 888 images belonging to 10 classes.\n",
      "Found 816 images belonging to 10 classes.\n",
      "Found 925 images belonging to 10 classes.\n",
      "Found 806 images belonging to 10 classes.\n",
      "25/25 [==============================] - 86s 3s/step - loss: 2.7596 - acc: 0.0888 - val_loss: 2.8195 - val_acc: 0.1187\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 78s 3s/step - loss: 2.6836 - acc: 0.1013 - val_loss: 2.9619 - val_acc: 0.1094\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 80s 3s/step - loss: 2.6853 - acc: 0.0997 - val_loss: 2.8956 - val_acc: 0.1055\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 77s 3s/step - loss: 2.6592 - acc: 0.1022 - val_loss: 2.8083 - val_acc: 0.1094\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 84s 3s/step - loss: 2.7308 - acc: 0.0822 - val_loss: 2.8687 - val_acc: 0.1030\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 78s 3s/step - loss: 2.6558 - acc: 0.0904 - val_loss: 3.0928 - val_acc: 0.0322\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 78s 3s/step - loss: 2.6958 - acc: 0.0940 - val_loss: 2.9689 - val_acc: 0.0875\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 78s 3s/step - loss: 2.6476 - acc: 0.1085 - val_loss: 2.8937 - val_acc: 0.1068\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 85s 3s/step - loss: 2.6492 - acc: 0.0922 - val_loss: 2.7619 - val_acc: 0.1055\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 97s 4s/step - loss: 2.6428 - acc: 0.1129 - val_loss: 2.8031 - val_acc: 0.1037\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 81s 3s/step - loss: 2.6592 - acc: 0.0935 - val_loss: 2.7995 - val_acc: 0.1017\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 81s 3s/step - loss: 2.6814 - acc: 0.1077 - val_loss: 2.7711 - val_acc: 0.1081\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 80s 3s/step - loss: 2.5976 - acc: 0.1097 - val_loss: 2.6513 - val_acc: 0.1120\n",
      "Epoch 14/20\n",
      " 6/25 [======>.......................] - ETA: 30s - loss: 2.5959 - acc: 0.0885"
     ]
    }
   ],
   "source": [
    "num_folds = 5 #1 to 10\n",
    "fold = 0\n",
    "for directory in fold_directories:\n",
    "    fold +=1\n",
    "    #RESET WEIGHTS!!\n",
    "    model.load_weights('raw_model.h5')\n",
    "    #\n",
    "    train_directories = list(set(fold_directories) - set([directory]))\n",
    "    test_directories = [directory]\n",
    "    print(\"Running fold {}, holding data from {} and training on the remaining {}\" \\\n",
    "          .format(fold,directory,len(train_directories)))\n",
    "    train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = train_directories,\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "    test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                       directories = test_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=approx_fold_size/batch_size,\n",
    "                              epochs=20,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=approx_fold_size/batch_size,\n",
    "                              shuffle=True, \n",
    "                              verbose=True)\n",
    "    model.save_weights('trained_model_fold_{}.h5'.format(fold))\n",
    "    if fold == num_folds:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
