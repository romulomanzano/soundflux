{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning from InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this step is to leverage existing state of the art models such as Ince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed some details from this example [https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3](https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=128\n",
    "img_width = 256\n",
    "approx_fold_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_sounds_folder = \"/home/nvidia/soundflux_data/spectrograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_directories = []\n",
    "for i in range(1,11):\n",
    "    directory = urban_sounds_folder+\"/fold\"+str(i)\n",
    "    fold_directories.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nvidia/soundflux_data/spectrograms/fold1',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold2',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold3',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold4',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold5',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold6',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold7',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold8',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold9',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold10']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-96b86e834f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_height' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import metrics\n",
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 10\n",
    "\n",
    "base_model = VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(base_model)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "#add_model.add(Dense(2000,activation='relu'))\n",
    "add_model.add(Dropout(0.5))\n",
    "#add_model.add(Dense(2000,activation='relu'))\n",
    "#add_model.add(Dense(2000,activation='relu'))\n",
    "#add_model.add(Dense(512,activation='relu'))\n",
    "add_model.add(Dense(nclass, \n",
    "                    activation='softmax'))\n",
    "\n",
    "#set optimizer\n",
    "#opt = optimizers.Adam(lr=0.05, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "opt = optimizers.RMSprop(lr=0.001)\n",
    "model = add_model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weights!\n",
    "model.save_weights('raw_inception_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1, holding data from /home/nvidia/soundflux_data/spectrograms/fold1 and training on the remaining 9\n",
      "Epoch 1/100\n",
      "Found 936 images belonging to 10 classes.\n",
      "Found 873 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Found 990 images belonging to 10 classes.\n",
      "Found 888 images belonging to 10 classes.\n",
      "Found 925 images belonging to 10 classes.\n",
      "Found 816 images belonging to 10 classes.\n",
      "Found 837 images belonging to 10 classes.\n",
      "Found 838 images belonging to 10 classes.\n",
      "Found 806 images belonging to 10 classes.\n",
      "225/225 [==============================] - 162s 719ms/step - loss: 2.6191 - acc: 0.2740 - val_loss: 4.8862 - val_acc: 0.1263\n",
      "Epoch 2/100\n",
      "225/225 [==============================] - 135s 598ms/step - loss: 2.4718 - acc: 0.3005 - val_loss: 6.3257 - val_acc: 0.0940\n",
      "Epoch 3/100\n",
      "225/225 [==============================] - 133s 593ms/step - loss: 2.4445 - acc: 0.2987 - val_loss: 5.0303 - val_acc: 0.1287\n",
      "Epoch 4/100\n",
      "225/225 [==============================] - 133s 592ms/step - loss: 2.3953 - acc: 0.3167 - val_loss: 5.8460 - val_acc: 0.1287\n",
      "Epoch 5/100\n",
      "225/225 [==============================] - 133s 593ms/step - loss: 2.3799 - acc: 0.3181 - val_loss: 5.1355 - val_acc: 0.1030\n",
      "Epoch 6/100\n",
      "225/225 [==============================] - 133s 591ms/step - loss: 2.3461 - acc: 0.3281 - val_loss: 5.7032 - val_acc: 0.1287\n",
      "Epoch 7/100\n",
      "225/225 [==============================] - 133s 593ms/step - loss: 2.3226 - acc: 0.3267 - val_loss: 5.3691 - val_acc: 0.1287\n",
      "Epoch 8/100\n",
      "223/225 [============================>.] - ETA: 1s - loss: 2.3140 - acc: 0.3306"
     ]
    }
   ],
   "source": [
    "num_folds = 10 #1 to 10\n",
    "fold = 0\n",
    "for directory in fold_directories:\n",
    "    fold +=1\n",
    "    #RESET WEIGHTS!!\n",
    "    model.load_weights('raw_inception_model.h5')\n",
    "    #\n",
    "    train_directories = list(set(fold_directories) - set([directory]))\n",
    "    test_directories = [directory]\n",
    "    print(\"Running fold {}, holding data from {} and training on the remaining {}\" \\\n",
    "          .format(fold,directory,len(train_directories)))\n",
    "    train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = train_directories,\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "    test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                       directories = test_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=approx_fold_size*9/batch_size,\n",
    "                              epochs=100,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=approx_fold_size/batch_size,\n",
    "                              shuffle=True, \n",
    "                              verbose=True)\n",
    "    model.save_weights('trained_inception_model_fold_{}.h5'.format(fold))\n",
    "    with open('training_inception_history_fold_{}.json'.format(fold), 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    save_plots(history,'training_inception_accuracy_plot_fold_{}.png'.format(fold),\n",
    "               'training_inception_lossaccuracy_plot_fold_{}.png'.format(fold))\n",
    "    if fold == num_folds:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
