{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning from InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this step is to leverage existing state of the art models such as Ince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed some details from this example [https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3](https://www.kaggle.com/kmader/transfer-learning-with-inceptionv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=128\n",
    "img_width = 256\n",
    "approx_fold_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_sounds_folder = \"/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_directories = []\n",
    "for i in range(1,11):\n",
    "    directory = urban_sounds_folder+\"/fold\"+str(i)\n",
    "    fold_directories.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold1',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold2',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold3',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold4',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold5',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold6',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold7',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold8',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold9',\n",
       " '/media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold10']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 2, 6, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2000)              4098000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000)              4002000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                20010     \n",
      "=================================================================\n",
      "Total params: 29,922,794\n",
      "Trainable params: 8,120,010\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras import applications\n",
    "from keras import metrics\n",
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 10\n",
    "\n",
    "base_model = applications.InceptionV3(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(base_model)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "add_model.add(Dense(2000,activation='relu'))\n",
    "add_model.add(Dropout(0.3))\n",
    "#add_model.add(Dense(2000,activation='relu'))\n",
    "add_model.add(Dense(2000,activation='relu'))\n",
    "#add_model.add(Dense(512,activation='relu'))\n",
    "add_model.add(Dense(nclass, \n",
    "                    activation='softmax'))\n",
    "\n",
    "#set optimizer\n",
    "#opt = optimizers.Adam(lr=0.05, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "opt = optimizers.SGD(lr=0.05, momentum=0.9,decay = 0.002)\n",
    "model = add_model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weights!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1, holding data from /media/nvidia/d171b53a-0021-4d77-b891-8d723bedc16d1/soundflux/urbansounds/spectrograms/fold1 and training on the remaining 9\n",
      "Epoch 1/100\n",
      "Found 888 images belonging to 10 classes.Found 873 images belonging to 10 classes.\n",
      "\n",
      "Found 837 images belonging to 10 classes.\n",
      "Found 990 images belonging to 10 classes.\n",
      "Found 936 images belonging to 10 classes.\n",
      "Found 816 images belonging to 10 classes.\n",
      "Found 806 images belonging to 10 classes.\n",
      "Found 925 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Found 838 images belonging to 10 classes.\n",
      "225/225 [==============================] - 159s 707ms/step - loss: 14.2681 - acc: 0.1111 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 2/100\n",
      "225/225 [==============================] - 129s 573ms/step - loss: 14.2555 - acc: 0.1156 - val_loss: 14.0437 - val_acc: 0.1287\n",
      "Epoch 3/100\n",
      "197/225 [=========================>....] - ETA: 14s - loss: 14.2465 - acc: 0.1161"
     ]
    }
   ],
   "source": [
    "num_folds = 10 #1 to 10\n",
    "fold = 0\n",
    "for directory in fold_directories:\n",
    "    fold +=1\n",
    "    #RESET WEIGHTS!!\n",
    "    model.load_weights('raw_model.h5')\n",
    "    #\n",
    "    train_directories = list(set(fold_directories) - set([directory]))\n",
    "    test_directories = [directory]\n",
    "    print(\"Running fold {}, holding data from {} and training on the remaining {}\" \\\n",
    "          .format(fold,directory,len(train_directories)))\n",
    "    train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = train_directories,\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "    test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                       directories = test_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=approx_fold_size*9/batch_size,\n",
    "                              epochs=100,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=approx_fold_size/batch_size,\n",
    "                              shuffle=True, \n",
    "                              verbose=True)\n",
    "    model.save_weights('trained_model_fold_{}.h5'.format(fold))\n",
    "    with open('training_history_fold_{}.json'.format(fold), 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    save_plots(history,'training_accuracy_plot_fold_{}.png'.format(fold),\n",
    "               'training_lossaccuracy_plot_fold_{}.png'.format(fold))\n",
    "    if fold == num_folds:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
