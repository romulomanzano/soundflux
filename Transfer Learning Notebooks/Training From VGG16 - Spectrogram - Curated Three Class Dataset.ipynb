{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urbansounds dataset proved to be very noisy. A lot of the classes were either irrelevant to our work or too similar for spectrograms to be distinguishable.\n",
    "\n",
    "We decided to narrow the focus of our modelling efforts by selecting a few categories\n",
    "\n",
    "This specific model uses three classes collected by the soundflux team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of March 28th, the classes used are as followes:\n",
    "\n",
    "- Falling Dummy (simulated human falls from Rescue Randy) from SoundFlux\n",
    "- General noise from SoundFlux\n",
    "- Falling Object (semi-bouncy object falling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from: https://s3.amazonaws.com/soundflux-urbansounds/curated_soundflux_three_classes.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "        print(generator.class_indices)\n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=80\n",
    "img_width = 256\n",
    "approx_fold_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/home/nvidia/curated_soundflux_three_classes/train/\"\n",
    "test_folder = \"/home/nvidia/curated_soundflux_three_classes/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            #rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.3,\n",
    "                            #horizontal_flip=True,\n",
    "                            #vertical_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\"\"\"   #Adding custom Layers\n",
    "   x = model.output\n",
    "   x = Flatten()(x)\n",
    "   x = Dense(4096, activation=\"relu\")(x)\n",
    "   x = Dropout(0.5)(x)\n",
    "   x = Dense(4096, activation=\"relu\")(x)\n",
    "   x = Dropout(0.5)(x)\n",
    "\"\"\"\n",
    "model = keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dense(64,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nclass, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 15,012,355\n",
      "Trainable params: 7,377,091\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.RMSprop(lr=0.0001,decay=1e-3)\n",
    "#opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weigh\"\"\"ts!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 291 images belonging to 3 classes.\n",
      "Found 137 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(test_folder,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'falling_dummy': 0, 'falling_object': 1, 'general_noise': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Found 291 images belonging to 3 classes.\n",
      "31/31 [============================>.] - ETA: 0s - loss: 0.7801 - acc: 0.6418Found 137 images belonging to 3 classes.\n",
      "32/31 [==============================] - 73s 2s/step - loss: 0.7704 - acc: 0.6477 - val_loss: 0.4692 - val_acc: 0.7946\n",
      "Epoch 2/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.4353 - acc: 0.8338 - val_loss: 0.3396 - val_acc: 0.8760\n",
      "Epoch 3/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.3305 - acc: 0.8802 - val_loss: 0.3213 - val_acc: 0.8725\n",
      "Epoch 4/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2718 - acc: 0.9086 - val_loss: 0.3139 - val_acc: 0.8899\n",
      "Epoch 5/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2589 - acc: 0.9111 - val_loss: 0.2807 - val_acc: 0.8950\n",
      "Epoch 6/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.2199 - acc: 0.9298 - val_loss: 0.4367 - val_acc: 0.8725\n",
      "Epoch 7/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.2115 - acc: 0.9429 - val_loss: 0.2966 - val_acc: 0.9061\n",
      "Epoch 8/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.1012 - acc: 0.9667 - val_loss: 0.7459 - val_acc: 0.8634\n",
      "Epoch 9/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.1075 - acc: 0.9677 - val_loss: 0.4219 - val_acc: 0.8883\n",
      "Epoch 10/20\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.0845 - acc: 0.9745 - val_loss: 0.2217 - val_acc: 0.9432\n",
      "Epoch 11/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.1036 - acc: 0.9726 - val_loss: 0.2599 - val_acc: 0.9199\n",
      "Epoch 12/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.0673 - acc: 0.9834 - val_loss: 0.1955 - val_acc: 0.9447\n",
      "Epoch 13/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.0697 - acc: 0.9824 - val_loss: 0.2543 - val_acc: 0.9293\n",
      "Epoch 14/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.0574 - acc: 0.9863 - val_loss: 0.3095 - val_acc: 0.9300\n",
      "Epoch 15/20\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.0785 - acc: 0.9843 - val_loss: 0.2539 - val_acc: 0.9289\n",
      "Epoch 16/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.0305 - acc: 0.9882 - val_loss: 1.1564 - val_acc: 0.7729\n",
      "Epoch 17/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.0666 - acc: 0.9853 - val_loss: 0.3848 - val_acc: 0.9176\n",
      "Epoch 18/20\n",
      "32/31 [==============================] - 49s 2s/step - loss: 0.0543 - acc: 0.9853 - val_loss: 0.3336 - val_acc: 0.9221\n",
      "Epoch 19/20\n",
      "32/31 [==============================] - 48s 1s/step - loss: 0.0551 - acc: 0.9912 - val_loss: 0.2163 - val_acc: 0.9432\n",
      "Epoch 20/20\n",
      "32/31 [==============================] - 48s 2s/step - loss: 0.0827 - acc: 0.9843 - val_loss: 0.2148 - val_acc: 0.9424\n"
     ]
    }
   ],
   "source": [
    "#RESET WEIGHTS!!\n",
    "#model.load_weights('raw_model.h5')\n",
    "#\n",
    "history = model.fit_generator(train_generator,\n",
    "                          steps_per_epoch=approx_fold_size/batch_size,\n",
    "                          validation_data = test_generator,\n",
    "                          validation_steps = approx_fold_size/batch_size,\n",
    "                          epochs=20,\n",
    "                          shuffle=True, \n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best so far -> lr = 0.0001, decay=1e-3, 13 epochs. Filal val accuracy = 92, loss = 0.4309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 137 images belonging to 3 classes.\n",
      "32/31 [==============================] - 23s 730ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21583733042128148, 0.9401805873783663]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "model.evaluate_generator(test_generator,\n",
    "                              steps=approx_fold_size/batch_size,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('curated_model_three_classes_unfrozen_layers_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"curated_model_three_classes_unfrozen_layers_v1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"curated_model_three_classes_unfrozen_layers_v1_class_indices.json\", \"w\") as json_file:\n",
    "    train_generator.class_indices\n",
    "    json_file.write(json.dumps(train_generator.class_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights were saved on the very first run of this model and ara available here: \n",
    "https://s3.amazonaws.com/soundflux-urbansounds/curated_model_three_classes_unfrozen_layers_v1.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
