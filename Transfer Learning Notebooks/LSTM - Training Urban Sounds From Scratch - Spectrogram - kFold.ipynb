{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this step is to leverage existing state of the art models such as Ince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,directories, batch_size, img_height,img_width):\n",
    "    generators =[]\n",
    "    for directory in directories:\n",
    "        gen = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "    \n",
    "        generators.append(gen)\n",
    "\n",
    "    for gen in generators:\n",
    "        for data, labels in gen:\n",
    "            yield data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=80\n",
    "img_width = 256\n",
    "approx_fold_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_sounds_folder = \"/home/nvidia/soundflux_data/spectrograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_directories = []\n",
    "for i in range(1,11):\n",
    "    directory = urban_sounds_folder+\"/fold\"+str(i)\n",
    "    fold_directories.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nvidia/soundflux_data/spectrograms/fold1',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold2',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold3',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold4',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold5',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold6',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold7',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold8',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold9',\n",
       " '/home/nvidia/soundflux_data/spectrograms/fold10']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            #rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.3,\n",
    "                            #horizontal_flip=True,\n",
    "                            #vertical_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "testdatagen = ImageDataGenerator(rescale=1./255)\n",
    "#datagen = ImageDataGenerator(horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next ideas:\n",
    "\n",
    "- Add filters on time with strides! 2,1 convolutions with padding and a gap\n",
    "- Keep at least a good 64 channels open?\n",
    "- Sum pooling instead of averaging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try \n",
    "\n",
    "keras.applications.densenet.DenseNet121 next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height,img_width,3)\n",
    "permute = (2,3,1)\n",
    "new_input_shape = (img_width,1,img_height,3)\n",
    "conv_input_shape = (img_width,1,img_height,3)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Permute(permute, input_shape=input_shape))\n",
    "#model.add(keras.layers.Reshape((1,1, img_width, 3*img_height), input_shape=(img_width,img_height,3)))\n",
    "model.add(keras.layers.Reshape(conv_input_shape, input_shape=new_input_shape))\n",
    "#\n",
    "#model.add(keras.layers.Conv3D(32,(2,1,2),activation='relu'))\n",
    "#model.add(keras.layers.AveragePooling3D((1,1,2)))\n",
    "model.add(keras.layers.ConvLSTM2D(64, kernel_size=(1, 4),activation='tanh',\n",
    "                                  input_shape=new_input_shape,\n",
    "                                   padding='same', return_sequences=True\n",
    "                                 ))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.AveragePooling3D((2,1,3)))\n",
    "model.add(keras.layers.Conv3D(32,(2,1,2),activation='relu',input_shape=input_shape))\n",
    "model.add(keras.layers.AveragePooling3D((2,1,3)))\n",
    "model.add(keras.layers.Conv3D(32,(2,1,2),activation='relu',input_shape=input_shape))\n",
    "#model.add(keras.layers.Conv2D(64,(2,2),activation='relu'))\n",
    "#model.add(keras.layers.AveragePooling2D((2,2)))\n",
    "#model.add(keras.layers.Conv2D(128,(1,2),activation='relu'))\n",
    "#model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nclass, activation=tf.nn.softmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 256, 3, 80)        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 256, 1, 80, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 256, 1, 80, 64)    68864     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256, 1, 80, 64)    256       \n",
      "_________________________________________________________________\n",
      "average_pooling3d (AveragePo (None, 128, 1, 26, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 127, 1, 25, 32)    8224      \n",
      "_________________________________________________________________\n",
      "average_pooling3d_1 (Average (None, 63, 1, 8, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 62, 1, 7, 32)      4128      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 13888)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13888)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 55556     \n",
      "=================================================================\n",
      "Total params: 137,028\n",
      "Trainable params: 136,900\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.RMSprop(lr=0.001)\n",
    "#opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weigh\"\"\"ts!\n",
    "model.save_weights('raw_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Sizing**\n",
    "This is helpful when understanding sizing\n",
    "https://stackoverflow.com/questions/50242925/data-preprocessing-input-shape-for-timedistributed-cnn-lrcn-convlstm2d-for?rq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore**\n",
    "- Should also be returning state?\n",
    "- Should also flip sizing? (channel representation should be 3 instead of the height?)\n",
    "- Run another LSTM from top to bottom to traverse from the spectrums/frequency bands from lower to higher!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/image-detection-from-scratch-in-keras-f314872006c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2, holding data from /home/nvidia/soundflux_data/spectrograms/fold2 and training on the remaining 9\n",
      "Epoch 1/30\n",
      "Found 248 images belonging to 4 classes.\n",
      "Found 248 images belonging to 4 classes.\n",
      "Found 245 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n",
      "Found 363 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 309 images belonging to 4 classes.\n",
      "Found 240 images belonging to 4 classes.\n",
      "Found 298 images belonging to 4 classes.\n",
      "84/84 [============================>.] - ETA: 1s - loss: 1.0592 - acc: 0.5905Found 268 images belonging to 4 classes.\n",
      "85/84 [==============================] - 429s 5s/step - loss: 1.0545 - acc: 0.5920 - val_loss: 1.2325 - val_acc: 0.6233\n",
      "Epoch 2/30\n",
      "85/84 [==============================] - 396s 5s/step - loss: 0.5553 - acc: 0.7787 - val_loss: 1.0464 - val_acc: 0.7400\n",
      "Epoch 3/30\n"
     ]
    }
   ],
   "source": [
    "num_folds = 1 #1 to 10\n",
    "fold = 0\n",
    "for directory in fold_directories:\n",
    "    fold +=1\n",
    "    #RESET WEIGHTS!!\n",
    "    #model.load_weights('raw_model.h5')\n",
    "    #\n",
    "    #FORCE A DIFFERENT FOLD OF THE DATA:\n",
    "    if directory == '/home/nvidia/soundflux_data/spectrograms/fold1':\n",
    "        continue\n",
    "    train_directories = list(set(fold_directories) - set([directory]))\n",
    "    test_directories = [directory]\n",
    "    #HACK IT\n",
    "    #train_directories = [\"/home/nvidia/soundflux_data/spectrograms/fold1\",\"/home/nvidia/soundflux_data/spectrograms/fold2\"]\n",
    "    #test_directories = [\"/home/nvidia/soundflux_data/spectrograms/fold3\"]\n",
    "    print(\"Running fold {}, holding data from {} and training on the remaining {}\" \\\n",
    "          .format(fold,directory,len(train_directories)))\n",
    "    train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = train_directories,\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "    test_generator = generate_generator_multiple(generator=testdatagen,\n",
    "                                       directories = test_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=approx_fold_size*9/batch_size,\n",
    "                              epochs=30,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=approx_fold_size/batch_size,\n",
    "                              shuffle=True, \n",
    "                              verbose=True)\n",
    "    model.save_weights('trained_model_fold_{}.h5'.format(fold))\n",
    "    with open('training_history_fold_{}.json'.format(fold), 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    save_plots(history,'training_accuracy_plot_fold_{}.png'.format(fold),\n",
    "               'training_lossaccuracy_plot_fold_{}.png'.format(fold))\n",
    "    if fold >= num_folds:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this: https://mc.ai/urban-sound-classification-using-convolutional-neural-networks-with-keras-theory-and/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = generate_generator_multiple(generator=testdatagen,\n",
    "                                       directories = test_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "#experiment with different data gen\n",
    "train_generator = generate_generator_multiple(generator=datagen,\n",
    "                                       directories = train_directories,\n",
    "                                       batch_size=batch_size,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "model.evaluate_generator(test_generator,\n",
    "                              steps=approx_fold_size/batch_size,\n",
    "                              verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
