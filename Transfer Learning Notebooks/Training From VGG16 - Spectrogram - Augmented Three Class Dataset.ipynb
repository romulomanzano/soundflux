{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urbansounds dataset proved to be very noisy. A lot of the classes were either irrelevant to our work or too similar for spectrograms to be distinguishable.\n",
    "\n",
    "We decided to narrow the focus of our modelling efforts by selecting a few categories\n",
    "\n",
    "This specific model uses two classes collected by the soundflux team. These classes (falling_dummy and falling_object) were augmented by overlaying them with random samples of seven different audio clips, ranging from a group talking in the background to the sound of running water in a shower. The idea was to make the model generalize well under unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of April 12th, the classes used are as follows:\n",
    "\n",
    "- Falling Dummy (simulated human falls from Rescue Randy) from SoundFlux plus all the augmented iterations\n",
    "- Overlay noise from SoundFlux used to augment\n",
    "- Falling Object (semi-bouncy object falling) plus all the augmented iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from: https://s3.amazonaws.com/soundflux-urbansounds/soundflux_augmented_three_class_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/media/romulo/6237-3231/curated_soundflux_three_classes/test\"def save_plots(history,target_file_acc,target_file_loss):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_acc)\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(target_file_loss)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height=80\n",
    "img_width = 256\n",
    "approx_fold_size = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/home/nvidia/Downloads/soundflux_augmented/spectrograms/split/train/\"\n",
    "test_folder = \"/home/nvidia/Downloads/soundflux_augmented/spectrograms/split/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            #rotation_range=10,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.3,\n",
    "                            #horizontal_flip=True,\n",
    "                            #vertical_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width,3)\n",
    "nclass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(img_height, img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\"\"\"   #Adding custom Layers\n",
    "\"\"\"\n",
    "model = keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dense(64,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nclass, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 15,012,355\n",
      "Trainable params: 7,377,091\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.RMSprop(lr=0.0001,decay=1e-3)\n",
    "#opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "#needed to reset weigh\"\"\"ts!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15861 images belonging to 3 classes.\n",
      "Found 4976 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(test_folder,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          seed=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'falling_dummy': 0, 'falling_object': 1, 'overlay_noise': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "282/281 [==============================] - 352s 1s/step - loss: 0.3381 - acc: 0.8633 - val_loss: 0.1059 - val_acc: 0.9590\n",
      "Epoch 2/20\n",
      " 63/281 [=====>........................] - ETA: 3:29 - loss: 0.1850 - acc: 0.9335"
     ]
    }
   ],
   "source": [
    "#RESET WEIGHTS!!\n",
    "#model.load_weights('raw_model.h5')\n",
    "#\n",
    "history = model.fit_generator(train_generator,\n",
    "                          steps_per_epoch=approx_fold_size/batch_size,\n",
    "                          validation_data = test_generator,\n",
    "                          validation_steps = 1000/batch_size,\n",
    "                          epochs=20,\n",
    "                          shuffle=True, \n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_generator = generate_generator_multiple(generator=datagen,\n",
    "                                           directories = [test_folder],\n",
    "                                           batch_size=batch_size,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width)\n",
    "model.evaluate_generator(test_generator,\n",
    "                              steps=approx_fold_size/batch_size,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('augmented_model_three_classes_unfrozen_layers_v1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Object URL\n",
    "https://s3.amazonaws.com/soundflux-urbansounds/augmented_three_classes_vgg16_retrained_model.zipmodel_json = model.to_json()\n",
    "with open(\"augmented_model_three_classes_unfrozen_layers_v1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"augmented_model_three_classes_unfrozen_layers_v1_class_indices.json\", \"w\") as json_file:\n",
    "    train_generator.class_indices\n",
    "    json_file.write(json.dumps(train_generator.class_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights were saved on the very first run of this model and ara available here: \n",
    "Object URL\n",
    "https://s3.amazonaws.com/soundflux-urbansounds/augmented_three_classes_vgg16_retrained_model.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
